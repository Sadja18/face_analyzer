{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2369889-a465-48d7-a761-001520056f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imutils\n",
    "# !pip install filetype\n",
    "# !pip install deepface\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ead3c26-e51b-4ece-9c2d-6c0825f85840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 18:31:52.092917: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-05 18:31:52.124497: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-05 18:31:52.125033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 18:31:52.637892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import filetype\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f0f8da-4fc6-4319-98df-ca6272af5537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 18:31:53.669185: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2023-12-05 18:31:53.734213: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2023-12-05 18:31:53.800619: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2023-12-05 18:31:54.365521: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2023-12-05 18:31:54.796510: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "Action: race: 100%|███████████████████████████████| 4/4 [00:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white\n",
      "happy\n",
      "35\n",
      "Woman\n",
      "profile\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|███████████████████████████████| 4/4 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian\n",
      "sad\n",
      "29\n",
      "Man\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def deep_face_analyzer(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    color_img_new = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #plt.imshow( color_img)\n",
    "\n",
    "    # this analyses the given image and gives values\n",
    "    # when we use this for 1st time, it may give many errors \n",
    "    # and some google drive links to download some '.h5' and zip files, \n",
    "    # download and save them in the location where it shows that files are missing.\n",
    "    prediction = DeepFace.analyze(color_img_new)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def view_it(image_path='download12.jpeg'):\n",
    "    a = deep_face_analyzer(image_path)\n",
    "    a= a[0]\n",
    "    print(a['dominant_race'])\n",
    "    print(a['dominant_emotion'])\n",
    "    print(a['age'])\n",
    "    print(a['dominant_gender'])\n",
    "\n",
    "view_it('./download12.jpeg')\n",
    "\n",
    "print('profile\\n\\n')\n",
    "view_it('./profile.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2770e606-3b3c-4bae-b203-bc953bea9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARGIN = 10  # pixels\n",
    "# ROW_SIZE = 10  # pixels\n",
    "# FONT_SIZE = 1\n",
    "# FONT_THICKNESS = 1\n",
    "# TEXT_COLOR = (255, 0, 0)  # red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33395a39-ffb9-4e84-9487-19032f30946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_FILE='profile.jpg'\n",
    "# cascPath = \"haarcascade_frontalface_default.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131b3b3e-63af-4a62-af93-5874a9891013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download from: \n",
    "# # https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "# FACE_MODEL = \"weights/res10_300x300_ssd_iter_140000_fp16.caffemodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feb3b2f6-4ddf-4ede-a3c2-ffedf69c172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The model architecture\n",
    "# # download from: https://drive.google.com/open?id=1kiusFljZc9QfcIYdU2s7xrtWHTraHwmW\n",
    "# AGE_MODEL = 'weights/deploy_age.prototxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44df7b71-45da-4430-bc8e-91c3249a1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The gender model architecture\n",
    "# # https://drive.google.com/open?id=1W_moLzMlGiELyPxWiYQJ9KFaXroQ_NFQ\n",
    "# GENDER_MODEL = 'weights/deploy_gender.prototxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f129ec-5516-4cfc-bed6-8d7d6844c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download from: https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n",
    "# FACE_PROTO = \"weights/deploy.prototxt.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb7eede2-9f2e-419b-81d7-e2f2aed2d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The model pre-trained weights\n",
    "# # download from: https://drive.google.com/open?id=1kWv0AjxGSN0g31OeJa02eBGM0R_jcjIl\n",
    "# AGE_PROTO = 'weights/age_net.caffemodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e27ed888-2368-4ffd-be21-a863bd6fcb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The gender model pre-trained weights\n",
    "# # https://drive.google.com/open?id=1AW3WduLk1haTVAxHOkVS_BEzel1WXQHP\n",
    "# GENDER_PROTO = 'weights/gender_net.caffemodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f89b4d-082d-4a08-b22b-2c6b230ca6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Represent the 8 age classes of this CNN probability layer\n",
    "# AGE_INTERVALS = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)',\n",
    "#                  '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afbf40df-256c-47a5-8c8d-b55d18c1ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Represent the gender classes\n",
    "# GENDER_LIST = ['Male', 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506b3b55-606c-4ba7-9e3a-301865c57960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Each Caffe Model impose the shape of the input image also image preprocessing is required like mean\n",
    "# # substraction to eliminate the effect of illunination changes\n",
    "# MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3b111fb-87e6-4f2e-b9ff-e251474ec5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize frame size\n",
    "# frame_width = 128\n",
    "# frame_height = 170\n",
    "# # load face Caffe model\n",
    "# face_net = cv2.dnn.readNetFromCaffe(FACE_PROTO, FACE_MODEL)\n",
    "\n",
    "# # Load age prediction model\n",
    "# age_net = cv2.dnn.readNetFromCaffe(AGE_MODEL, AGE_PROTO)\n",
    "\n",
    "# # Load gender prediction model\n",
    "# gender_net = cv2.dnn.readNetFromCaffe(GENDER_MODEL, GENDER_PROTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ac0797-9282-47f3-8ba7-823b89eb025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_faces(frame, confidence_threshold=0.5):\n",
    "#     \"\"\"Returns the box coordinates of all detected faces\"\"\"\n",
    "#     # convert the frame into a blob to be ready for NN input\n",
    "#     blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104, 177.0, 123.0))\n",
    "#     # set the image as input to the NN\n",
    "#     face_net.setInput(blob)\n",
    "#     # perform inference and get predictions\n",
    "#     output = np.squeeze(face_net.forward())\n",
    "#     # initialize the result list\n",
    "#     faces = []\n",
    "#     # Loop over the faces detected\n",
    "#     for i in range(output.shape[0]):\n",
    "#         confidence = output[i, 2]\n",
    "#         if confidence > confidence_threshold:\n",
    "#             box = output[i, 3:7] * np.array([frame_width, frame_height, frame_width, frame_height])\n",
    "#             # convert to integers\n",
    "#             start_x, start_y, end_x, end_y = box.astype(int)\n",
    "#             # widen the box a little\n",
    "#             start_x, start_y, end_x, end_y = start_x - \\\n",
    "#                 10, start_y - 10, end_x + 10, end_y + 10\n",
    "#             start_x = 0 if start_x < 0 else start_x\n",
    "#             start_y = 0 if start_y < 0 else start_y\n",
    "#             end_x = 0 if end_x < 0 else end_x\n",
    "#             end_y = 0 if end_y < 0 else end_y\n",
    "#             # append to our list\n",
    "#             faces.append((start_x, start_y, end_x, end_y))\n",
    "#     return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84236e99-ac55-4cd2-97a9-abf9601d8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cv2_imshow(title, img):\n",
    "#     \"\"\"Displays an image on screen and maintains the output until the user presses a key\"\"\"\n",
    "#     # Display Image on screen\n",
    "#     cv2.imshow(title, img)\n",
    "#     # Mantain output until user presses a key\n",
    "#     cv2.waitKey(0)\n",
    "#     # Destroy windows when user presses a key\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75f9dfef-df00-4018-b5ed-e7b1568641b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_optimal_font_scale(text, width):\n",
    "#     \"\"\"Determine the optimal font scale based on the hosting frame width\"\"\"\n",
    "#     for scale in reversed(range(0, 60, 1)):\n",
    "#         textSize = cv2.getTextSize(text, fontFace=cv2.FONT_HERSHEY_PLAIN , fontScale=scale/10, thickness=1)\n",
    "#         new_width = textSize[0][0]\n",
    "#         if (new_width <= width):\n",
    "#             # print(scale/10)\n",
    "#             return scale/10\n",
    "#         # print(new_width)\n",
    "#     return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6423489-516b-467e-ae31-5a12b7f2c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from: https://stackoverflow.com/questions/44650888/resize-an-image-without-distortion-opencv\n",
    "# def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "#     # initialize the dimensions of the image to be resized and\n",
    "#     # grab the image size\n",
    "#     dim = None\n",
    "#     (h, w) = image.shape[:2]\n",
    "#     # if both the width and height are None, then return the\n",
    "#     # original image\n",
    "#     if width is None and height is None:\n",
    "#         return image\n",
    "#     # check to see if the width is None\n",
    "#     if width is None:\n",
    "#         # calculate the ratio of the height and construct the\n",
    "#         # dimensions\n",
    "#         r = height / float(h)\n",
    "#         dim = (int(w * r), height)\n",
    "#     # otherwise, the height is None\n",
    "#     else:\n",
    "#         # calculate the ratio of the width and construct the\n",
    "#         # dimensions\n",
    "#         r = width / float(w)\n",
    "#         dim = (width, int(h * r))\n",
    "#     # resize the image\n",
    "#     return cv2.resize(image, dim, interpolation = inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08682b59-274b-4195-8d13-c961f18ffe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_age(input_path: str):\n",
    "#     \"\"\"Predict the age of the faces showing in the image\"\"\"\n",
    "#     # Read Input Image\n",
    "#     img = cv2.imread(input_path)\n",
    "#     # Take a copy of the initial image and resize it\n",
    "#     frame = img.copy()\n",
    "#     if frame.shape[1] > frame_width:\n",
    "#         frame = image_resize(frame, width=frame_width)\n",
    "#     faces = get_faces(frame)\n",
    "#     for i, (start_x, start_y, end_x, end_y) in enumerate(faces):\n",
    "#         face_img = frame[start_y: end_y, start_x: end_x]\n",
    "#         # image --> Input image to preprocess before passing it through our dnn for classification.\n",
    "#         blob = cv2.dnn.blobFromImage(\n",
    "#             image=face_img, scalefactor=1.0, size=(227, 227), \n",
    "#             mean=MODEL_MEAN_VALUES, swapRB=False\n",
    "#         )\n",
    "#         # Predict Age\n",
    "#         age_net.setInput(blob)\n",
    "#         age_preds = age_net.forward()\n",
    "#         print(\"=\"*30, f\"Face {i+1} Prediction Probabilities\", \"=\"*30)\n",
    "#         for i in range(age_preds[0].shape[0]):\n",
    "#             print(f\"{AGE_INTERVALS[i]}: {age_preds[0, i]*100:.2f}%\")\n",
    "#         i = age_preds[0].argmax()\n",
    "#         age = AGE_INTERVALS[i]\n",
    "#         age_confidence_score = age_preds[0][i]\n",
    "#         # Draw the box\n",
    "#         label = f\"Age:{age} - {age_confidence_score*100:.2f}%\"\n",
    "#         # print(label)\n",
    "#         # get the position where to put the text\n",
    "#         yPos = start_y - 15\n",
    "#         while yPos < 15:\n",
    "#             yPos += 15\n",
    "#         # write the text into the frame\n",
    "#         cv2.putText(frame, label, (start_x, yPos),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), thickness=1)\n",
    "#         # draw the rectangle around the face\n",
    "#         cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), color=(0, 0, 0), thickness=1)\n",
    "#     # Display processed image\n",
    "#     cv2_imshow('Age Estimator', frame)\n",
    "#     # save the image if you want\n",
    "#     # cv2.imwrite(\"predicted_age.jpg\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3272c8de-0f6d-4e31-b7c0-1b6ae081b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_gender(input_path: str):\n",
    "#     \"\"\"Predict the gender of the faces showing in the image\"\"\"\n",
    "#     # Read Input Image\n",
    "#     img = cv2.imread(input_path)\n",
    "#     # resize the image, uncomment if you want to resize the image\n",
    "#     # img = cv2.resize(img, (frame_width, frame_height))\n",
    "#     # Take a copy of the initial image and resize it\n",
    "#     frame = img.copy()\n",
    "#     if frame.shape[1] > frame_width:\n",
    "#         frame = image_resize(frame, width=frame_width)\n",
    "#     # predict the faces\n",
    "#     faces = get_faces(frame)\n",
    "#     # Loop over the faces detected\n",
    "#     # for idx, face in enumerate(faces):\n",
    "#     for i, (start_x, start_y, end_x, end_y) in enumerate(faces):\n",
    "#         face_img = frame[start_y: end_y, start_x: end_x]\n",
    "#         # image --> Input image to preprocess before passing it through our dnn for classification.\n",
    "#         # scale factor = After performing mean substraction we can optionally scale the image by some factor. (if 1 -> no scaling)\n",
    "#         # size = The spatial size that the CNN expects. Options are = (224*224, 227*227 or 299*299)\n",
    "#         # mean = mean substraction values to be substracted from every channel of the image.\n",
    "#         # swapRB=OpenCV assumes images in BGR whereas the mean is supplied in RGB. To resolve this we set swapRB to True.\n",
    "#         blob = cv2.dnn.blobFromImage(image=face_img, scalefactor=1.0, size=(\n",
    "#             227, 227), mean=MODEL_MEAN_VALUES, swapRB=False, crop=False)\n",
    "#         # Predict Gender\n",
    "#         gender_net.setInput(blob)\n",
    "#         gender_preds = gender_net.forward()\n",
    "#         i = gender_preds[0].argmax()\n",
    "#         gender = GENDER_LIST[i]\n",
    "#         gender_confidence_score = gender_preds[0][i]\n",
    "#         # Draw the box\n",
    "#         label = \"{}-{:.2f}%\".format(gender, gender_confidence_score*100)\n",
    "#         # print(label)\n",
    "#         yPos = start_y - 15\n",
    "#         while yPos < 15:\n",
    "#             yPos += 15\n",
    "#         # get the font scale for this image size\n",
    "#         optimal_font_scale = get_optimal_font_scale(label,((end_x-start_x)+25))\n",
    "#         box_color = None\n",
    "#         box_color = (0, 0, 0)\n",
    "#         cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), color=box_color, thickness=1)\n",
    "#         # Label processed image\n",
    "#         cv2.putText(frame, label, (start_x, yPos),\n",
    "#                     cv2.FONT_HERSHEY_PLAIN , optimal_font_scale, color=box_color, thickness=1)\n",
    "\n",
    "#         # Display processed image\n",
    "#     cv2_imshow(\"Gender Estimator\", frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46fa03d9-d454-49c6-8000-815d029cae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_age('./profile.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "408ebd48-b6df-496a-8ff3-8a1f2653581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_gender('./profile.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
